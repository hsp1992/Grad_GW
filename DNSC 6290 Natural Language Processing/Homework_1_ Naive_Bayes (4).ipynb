{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the homework to Blackboard by Sunday November 21st 11:59 pm. Make sure that all code blocks in the notebook run and are in order. Before you submit, under the Kernel menu, click **Restart & Run All**. Feel free to use any code from class without citation, but any code used from the internet include comments and citaiton. Use the Yelp data from Lecture_02."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Yelp data as we did in class. Before cleaning the data use `.apply(len)` to get the character count and `.apply(lambda x: len(x.split()))` to get the 'word' count for each review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.)** Plot Histograms or Density plots of the character and word counts also use `.describe()`. Comment on the plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.)** Print all 1 and 2 word reviews. Comment on the reviews and what effect including these in the training and test data might have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.)** Apply the clean function from class, overwrite 'text' or assign a new variable 'clean_text' that will be used to answer all questions moving forward. Rerun the 'word' counts. Print all 1 and 2 word reviews again, what are the differences? Are there any changes that could be made to the cleaning that might keep additional signals of the reviews sentiment?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.)** Compare the plots for word counts of reviews recieving 1 or 2 stars to those 3 to 4 stars. Do you notice any difference? If you do how might this affect analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Establish a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sklearn.model_selection.train_test_split()` with `random_state = 123` to create stratified train(80%) and test(20%) with positive (4 and 5 star) and negative (1 and 2 star) as the target.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.)** What is an absolute baseline (in that it is bad if a model underperforms this) and why?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.)** List sklearns default parameters for MultinomialNB to create another baseline. What are these values? Comment on its performance relative to part **a**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will be sequentially tuning a few of the parameters from the pipeline starting with a config of:\n",
    "```python\n",
    "config = {'vect__ngram_range': (1,1),\n",
    "          'vect__norm': 'l2',\n",
    "          'vect__lowercase': True,\n",
    "          'vect__use_idf': True,\n",
    "          'vect__min_df': 1,\n",
    "          'vect__stop_words': stp_wrds}\n",
    "```\n",
    "\n",
    "also add the following line prior to the return statement in `whole_pipeline()`:  \n",
    "```python\n",
    "print(f'Vocabulary size: {len(NB_classifier.named_steps.vect.vocabulary_):,}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.)** Fit multiple models using a range of smoothing values, $\\alpha \\in [.01, .1, 1, 10, 100]$. Use the `whole_pipeline()` function and comment on the results of each.  Use the best parameter for answering parts **c** and **d**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.)** For $\\alpha \\in [.01, 100]$ create histograms of the word coefficients (see suplemental material for an example). Comments on the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.)** Try each combination of **vect__min_df** $\\in [1,3,5]$ and **vect__ngram_range** $\\in [(1,1), (1,2)]$. Comment on changes in accuracy and vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Walk through an Example\n",
    "Choose a mislabeled sentence from the test set with at least 7 tokens. Find the coefficients of the terms within the sentence. Comment on why the sentence was mislabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
